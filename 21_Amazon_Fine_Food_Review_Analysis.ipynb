{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fine Food Review Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "    1. Id\n",
    "    2. ProductID - unique identifier for the product\n",
    "    3. UserID - unique identifier for the user\n",
    "    4. ProfileName\n",
    "    5. HelpfulnessNumerator - number of users who found the review useful\n",
    "    6. HelpfulnessDenominator - number of users indicating whether they found the review helpful or not\n",
    "    7. Score - rating between 1 & 5\n",
    "    8. Time - timestamp of the review\n",
    "    9. Summary - brief summary of the review\n",
    "    10. Text - text of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jayraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, determine whether the review is positive (Rating of 4 / 5) or negative (Rating of 1 / 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQLite Table to read the data\n",
    "con =sqlite3.connect('/Users/jayraj/Applied_AI_Course/Applied_ai_course/Datasets/Amazon Fine Dine Reviews/database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only positive and negative reviews i.e.\n",
    "# Not taking into considerations those reviews with Score=3\n",
    "filtered_data=pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM REVIEWS\n",
    "WHERE Score !=3\n",
    "\"\"\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to give reviews with score > 3, a positive rating, and reviews with a score < 3, a negative rating \n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the scores with score less than 3 positive and vice versa\n",
    "actualScore=filtered_data['Score']\n",
    "positiveNegative=actualScore.map(partition)\n",
    "filtered_data['Score']=positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
      "0                     1                       1  positive  1303862400   \n",
      "1                     0                       0  negative  1346976000   \n",
      "2                     1                       1  positive  1219017600   \n",
      "3                     3                       3  negative  1307923200   \n",
      "4                     0                       0  positive  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "filtered_data\n",
    "print(filtered_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and deduplication of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, when we do machine learning we spend 20% to 30% time in Data Cleaning & Preprocessing. \n",
    "\n",
    "The Dataset has many duplicate rows.\n",
    "\n",
    "Hence it is necessary to remove duplicates in order to get unbiased results for the analysis of the data. It is not adding any value to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
      "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
      "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
      "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
      "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
      "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
      "\n",
      "   HelpfulnessDenominator  Score        Time  \\\n",
      "0                       2      5  1199577600   \n",
      "1                       2      5  1199577600   \n",
      "2                       2      5  1199577600   \n",
      "3                       2      5  1199577600   \n",
      "4                       2      5  1199577600   \n",
      "\n",
      "                             Summary  \\\n",
      "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
      "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
      "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
      "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
      "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
      "\n",
      "                                                Text  \n",
      "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
      "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
      "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
      "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
      "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
      "(5, 10)\n"
     ]
    }
   ],
   "source": [
    "display=pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score !=3 AND UserId='AR5J8UI46CURR'\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "print(display)\n",
    "print(display.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Sorting the data according to the product Id in ascending order</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "138688  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "138689  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "138690  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "138691  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "138706                     0                       0  positive   939340800   \n",
       "138688                     1                       1  positive  1194739200   \n",
       "138689                     1                       1  positive  1191456000   \n",
       "138690                     1                       1  positive  1076025600   \n",
       "138691                     3                       4  positive  1018396800   \n",
       "\n",
       "                                           Summary  \\\n",
       "138706                   EVERY book is educational   \n",
       "138688  Love the book, miss the hard cover version   \n",
       "138689               chicken soup with rice months   \n",
       "138690      a good swingy rhythm for reading aloud   \n",
       "138691             A great way to learn the months   \n",
       "\n",
       "                                                     Text  \n",
       "138706  this witty little book makes my son laugh at l...  \n",
       "138688  I grew up reading these Sendak books, and watc...  \n",
       "138689  This is a fun way for children to learn their ...  \n",
       "138690  This is a great little book to read aloud- it ...  \n",
       "138691  This is a book of poetry about the months of t...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data=filtered_data.sort_values(\"ProductId\", axis=0, ascending=True)\n",
    "sorted_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deduplication of entries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=sorted_data.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Text'}, keep='first', inplace=False)\n",
    "final.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how much % of data still remains\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "69% of data is remaining after removing the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id   ProductId          UserId              ProfileName  \\\n",
      "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
      "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     3                       1      5  1224892800   \n",
      "1                     3                       2      4  1212883200   \n",
      "\n",
      "                                        Summary  \\\n",
      "0             Bought This for My Son at College   \n",
      "1  Pure cocoa taste with crunchy almonds inside   \n",
      "\n",
      "                                                Text  \n",
      "0  My son loves spaghetti so I didn't hesitate or...  \n",
      "1  It was almost a 'love at first bite' - the per...  \n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "display=pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score !=3 AND Id=44737 OR Id=64422\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "print(display)\n",
    "print(display.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Observation 2</h4> HelpfulnessNumerator should always be lesser than HelpfulnessDenominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    }
   ],
   "source": [
    "# Keeping the data wherein HelpfulnessNumerators<=HelpfulnessDenominator\n",
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.Score.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Next Phase of Preprocessing</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the 8 features as input we have to predict the sentiment polarity (+ve/-ve).\n",
    "We are determining the polarity by the Score.\n",
    "\n",
    "The most useful features are Summary and Text.\n",
    "\n",
    "Given any problem if we can convert into the problem of vectors, we can leverage the power of Linear Algebra.\n",
    "\n",
    "How do you convert text into numerical vectors?\n",
    "\n",
    "Convert Review Text into d-D vector in d-D space.\n",
    "\n",
    "Suppose we have many vectors. Each point represents a d-D representation of a review in d-D space.\n",
    "\n",
    "Draw a hyperplane 'pi' separating all positive reviews and all negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Converting Review-text into a d-D vector\n",
    "2. Finding a plane to separate the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Rules/Properties of this conversion </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have 3 reviews - $r_{1}$, $r_{2}$ and $r_{3}$. d-D representation of vectors for $r_{1}$ -> $v_{1}$, $r_{2}$ -> $v_{2}$, $r_{3}$ -> $v_{3}$.\n",
    "\n",
    "If $r_{1}$ and $r_{2}$ are more similar semantically that $r_{1}$ and $r_{3}$, i.e. Eng. sim($r_{1}$, $r_{2}$) > Eng. sim($r_{1}$, $r_{3}$) then the dist($v_{1}$, $v_{2}$) < dist($v_{1}$, $v_{3}$). \n",
    "\n",
    "If $r_{1}$ & $r_{2}$ are more similar, $v_{1}$ and $v_{2}$ must be close i.e. <b>length($v_{1}$  - $v_{2}$) < length($v_{1}$ - $v_{3}$)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>find {text -> d-D vector} such that similiar text must be closer geometrically.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest Technique to convert text to a numerical vector is <b>Bag Of Words(BoW)</b>\n",
    "\n",
    "$r_{1}$: This pasta is very tasty and affordable.\n",
    "\n",
    "$r_{2}$: This pasta is not tasty and is affordable.\n",
    "\n",
    "$r_{3}$: This pasta is delicious and cheap.\n",
    "\n",
    "$r_{4}$: Pasta is tasty and pasta tastes good.\n",
    "\n",
    "In NLP, a review is known as a document. Set of documents is called<b> corpus</b>.\n",
    "\n",
    "1. Constructing a dictionary - set of all unique words in the reviews. \n",
    "{This, pasta, ...}\n",
    "2. Construct Vector $v_{i}$ of size 'd'. Each word is a different dimension and each cell corresponds to # of times the word occurs in the review/document $r_{i}$.\n",
    "\n",
    "$v_{i}$ is a sparse vector - most of the elements are zero.\n",
    "\n",
    "<b>Objective of BoW:</b> Similar text must result as closer vectors.\n",
    "\n",
    "BoW is thought of counting the common words when all the values exist only once. How many common words exist? \n",
    "\n",
    "BoW does not work very well when there are small changes in the terminology we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Binary BoW</b> or <b>Boolean BoW</b> is a variation of BoW. Instead of putting count, we put 1 if the word occurs atleast once and 0 if the word doesn't exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||$v_{1}$ - $v_{2}$|| = $\\sqrt number of different words$ between documents/reviews $r_{1}$ and $r_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words like {This, is, and} do not matter much. What matters the most is the non-trivial words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the trivial words <b>Stop-words</b>.\n",
    "\n",
    "If I remove the Stop-words, BoW vector will be smaller and more meaningful. You throw these Stop-words while constructing the vector.\n",
    "\n",
    "In English 'not' is also considered as a Stop-word.\n",
    "\n",
    "<b>Note: So, removing the stop-words is not always the best choice.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Text Pre-processing steps</h4>\n",
    "1. Removing <b>Stop-words</b>.\n",
    "2. Convert all your words <b>lowercase</b>.\n",
    "3. <b>Stemming</b>: words coming from the same base word in English. Eg. tastes, tasful, tasty -> tast. Convert all these words into their common form i.e. taste and replace them with the common form. Related words are considered as single root word.\n",
    "Stemming algorithms - PorterStemmer, SnowballStemmer \n",
    "4. <b>Lemmatization</b>: breaking up a sentence into words. A space is used to break the sentence into words. \n",
    "Eg. This pasta is very tasty. This is the best in New York.\n",
    "But there can be complex words like New York. It is a location. \n",
    "Often times we break the sentence but there are lemmatizers available which will group New York into 1 word. \n",
    "5. Tasty and delicious are synonyms - very similar in meaning. But in BoW, we are considering them as 2 different words which are nowhere related because they are 2 different dimensions. In BoW we are not taking semantic meaning of words into consideration. A technique called <b>Word2Vec</b> where we try to get semantic meaning of these words into consideration when we build vectors of text.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>BoW + Text Preprocessing </b>\n",
    "Converting text to a d-D vector which doesn't guarantee semantic meaning of words will be at the same place. \n",
    "$r_{1}$ and $r_{3}$ are semantically same because our algorithm still think they are different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The <u>drawback</u> of BoW is <i>it doesn't take semantic meaning into consideration</i>.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect=CountVectorizer() # scikit learn\n",
    "final_counts=count_vect.fit_transform(final.Text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Example to understand countvectorizer() </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n",
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    "X=count_vect.fit_transform(corpus)\n",
    "print(count_vect.get_feature_names())\n",
    "print(X.toarray())\n",
    "\n",
    "count_vect2=CountVectorizer(analyzer='word', ngram_range=(2,2)) # bigrams\n",
    "Y=count_vect2.fit_transform(corpus)\n",
    "print(count_vect2.get_feature_names())\n",
    "print(Y.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a sparse matrix (dictionary). The space from (m x n) has been reduced to space (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.shape\n",
    "# 364171 rows and 115281 column containing bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Stemming, stop word removal and Lemmatization\n",
    "- Removal of html tags\n",
    "- Remove any punctuations or limited set of special characters like, or. or # etc\n",
    "- Check if the word is made up of english letters and isnot alpha numeric\n",
    "- Check to see if the length of word is greater than 2 \n",
    "- Convert the words to lowercase\n",
    "- Remove stopwords\n",
    "- Finally snowball stemming the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "I set aside at least an hour each day to read to my son (3 y/o). At this point, I consider myself a connoisseur of children's books and this is one of the best. Santa Clause put this under the tree. Since then, we've read it perpetually and he loves it.<br /><br />First, this book taught him the months of the year.<br /><br />Second, it's a pleasure to read. Well suited to 1.5 y/o old to 4+.<br /><br />Very few children's books are worth owning. Most should be borrowed from the library. This book, however, deserves a permanent spot on your shelf. Sendak's best.\n"
     ]
    }
   ],
   "source": [
    "# To find sentences containing HTML tags\n",
    "import re\n",
    "i=0\n",
    "for sent in final['Text'].values:\n",
    "    if(len(re.findall('<.*?>', sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we', 'who', 'why', 'between', 'o', 'you', 'own', \"shouldn't\", 'no', 'up', 'm', 'hadn', 'been', 'a', 'your', 'have', 'the', 'off', 'myself', 'there', 'for', 'wouldn', 'by', 'until', 'ain', 'having', 'am', 'this', 'nor', 'as', 'that', 're', 'how', 'mustn', 'they', 'were', 'about', 'yourselves', 'too', 'wasn', 'because', 'or', 'but', 'of', 'its', 'my', 't', 'him', 'below', 'don', 'more', 'few', \"wasn't\", 'i', 'some', \"needn't\", 'be', 'further', 'an', 'itself', 'very', 'do', 's', 'can', 'being', \"it's\", 'down', \"should've\", 'isn', 'whom', \"that'll\", 'against', 'hasn', 'when', 'has', \"wouldn't\", 'while', 'before', 'did', 'couldn', 'd', 'yourself', 'y', 'our', 'it', 'such', \"haven't\", 'will', 'here', \"mustn't\", \"she's\", 'through', 'theirs', 'are', 'in', 'any', 'shouldn', 'herself', 'she', \"couldn't\", 'what', 'shan', 'themselves', 'each', \"doesn't\", 'to', 'so', 'doesn', 'ma', 'now', \"mightn't\", \"hadn't\", 'doing', 'aren', 'yours', 'is', 'other', \"isn't\", 'does', 'needn', 'them', 'was', 'out', 'all', \"you're\", 'only', 'their', 'and', 'haven', 'at', 'most', 've', 'if', \"aren't\", 'mightn', \"you'd\", 'than', \"weren't\", 'from', 'me', 'his', 'll', 'then', \"you'll\", 'ourselves', 'where', 'same', \"won't\", 'during', 'on', 'under', \"didn't\", 'didn', 'not', \"shan't\", 'into', 'just', 'above', 'hers', 'after', 'he', 'those', 'had', 'once', 'over', 'himself', \"don't\", 'should', 'with', 'ours', 'both', 'weren', 'won', \"hasn't\", 'her', 'again', \"you've\", 'which', 'these'}\n",
      "************************************\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Set of stop words\n",
    "stop=set(stopwords.words('english')) \n",
    "\n",
    "# Initializing the snowball stemming\n",
    "sno=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def cleanhtml(sentence):\n",
    "    cleanr=re.compile('<.*?')\n",
    "    cleantext=re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence):\n",
    "    cleaned=re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned=re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return cleaned\n",
    "print(stop)\n",
    "print('************************************')\n",
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for implementing step-by-step preprocessing\n",
    "\n",
    "i = 0\n",
    "str1 = ' '\n",
    "final_string = []\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "s = ''\n",
    "for sentence in final['Text'].values:\n",
    "    filtered_sentence = []\n",
    "    #print(sentence)\n",
    "    sentence = cleanhtml(sentence)\n",
    "    for w in sentence.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if(final['Score'].values)[i] == 'positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if(final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(filtered_sentence)\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CleanedText column addition\n",
    "final['CleanedText']=final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final table into an SQlite table for future\n",
    "conn=sqlite3.connect('final.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory=str\n",
    "final.to_sql('Reviews', conn, schema=None, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi Grams and n-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_{1}$: This pasta is very tasty and affordable.\n",
    "\n",
    "$r_{2}$: This pasta is not tasty and is affordable.\n",
    "\n",
    "After removing stop-words $v_{1}$ and $v_{2}$ are exactly the same => $r_{1}$ and $r_{2}$ are very similar which is not TRUE. \n",
    "\n",
    "$r_{1}$ and $r_{2}$ are completely opposite. \n",
    "\n",
    "<b>Uni-gram</b>: Each word is considered as a dimension.\n",
    "<b>Bi-gram</b>: Pairs of consecutive words is considered as a dimension.\n",
    "<b>Tri-gram</b>: 3 consecutive words is considered as a dimension.\n",
    "<b>n-gram</b>: n consecutive words is considered as a dimension.\n",
    "\n",
    "<b>Why n-gram?</b> Uni-gram based BoW discards the sequence information. But using bi-gram, tri-gram or n-gram we are trying to retain some of the partial sequence information. \n",
    "\n",
    "Bi-gram, Tri-gram or n-gram can be easily encorporated into BoW.  \n",
    "\n",
    "<b># of bi-grams >= # of uni-grams</b> because the number of pairs of consecutive words is greater than or equal to uni-grams. \n",
    "\n",
    "<b># of n-grams >= ... >= # of tri-grams >= # of bi-grams >= # of uni-grams</b>\n",
    "\n",
    "For n-grams, where n > 1, dimensionality 'd' increases drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common positive words:  [(b'like', 139150), (b'tast', 128631), (b'good', 112216), (b'flavor', 109473), (b'love', 107034), (b'use', 103627), (b'great', 102818), (b'product', 99504), (b'one', 95360), (b'tri', 86237), (b'tea', 83824), (b'coffe', 78610), (b'make', 74835), (b'get', 71962), (b'food', 64752), (b'amazon', 57832), (b'would', 55297), (b'time', 55225), (b'buy', 53903), (b'realli', 52569)]\n",
      "Most common negative words:  [(b'tast', 34489), (b'like', 32284), (b'product', 29504), (b'one', 20420), (b'flavor', 19561), (b'would', 17901), (b'tri', 17676), (b'use', 15275), (b'good', 14977), (b'coffe', 14677), (b'get', 13758), (b'buy', 13690), (b'order', 12846), (b'food', 12742), (b'dont', 11683), (b'tea', 11657), (b'amazon', 11258), (b'even', 10983), (b'box', 10841), (b'make', 9816)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most common positive words: \", freq_dist_positive.most_common(20))\n",
    "print(\"Most common negative words: \", freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results positive and negative words overlap so it is better to consider pair of words i.e. bigrams, trigrams and n grams of words must be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-gram, tri-gram and n-gram\n",
    "\n",
    "# Removing stop words like \"not\" should be avoided before building n-grams\n",
    "count_vect=CountVectorizer(ngram_range=(1,2)) # in Scikit learn\n",
    "final_bigram_counts=count_vect.fit_transform(final['Text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigram_counts.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume we have 'N' documents / reviews. Each review is a combination of words.\n",
    "\n",
    "Let us assume $r_{1}$ has some words. Similarly, other documents too.\n",
    "\n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{3}$, $W_{2}$, $W_{5}$             --> 5 words\n",
    "\n",
    "$r_{2}$: $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$, $W_{6}$, $W_{2}$    --> 6 words\n",
    "\n",
    "$r_{3}$: \n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "$r_{N}$:\n",
    "\n",
    "TF($W_{i}$, $r_{j}$) = # of times $W_{i}$ occurs in $r_{j}$ / total number of words in $r{j}$\n",
    "TF($W_{2}$, $r_{1}$) = 2 / 5\n",
    "\n",
    "<b>0 <= TF($W_{i}$, $r_{j}$) <= 1 </b> Can be interpreted as Probability.\n",
    "\n",
    "<u>BoW</u> and <u>TF-IDF</u> are techniques done on the text for <i><b>Information Retrieval</b></i> (sub-area of NLP).\n",
    "\n",
    "TF can be thought of as how often does $W_{i}$ occur in $r_{j}$. If it has all the same words then it has a TF of 1 else if the word occurs a very few times, the TF has a very small value. <b> More often the word occurs, the higher the frequency. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Term Frequency can be thought of as the probability of finding a word $W_{i}$ in a document $r_{j}$. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>IDF- Inverse Document Frequency</i></b> is for a word $W_{i}$ in a corpus.\n",
    "\n",
    "Suppose Dataset/Corpus ($D_{c}$) has the following documents:\n",
    " \n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{3}$, $W_{2}$, $W_{5}$             --> 5 words\n",
    "\n",
    "$r_{2}$: $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$, $W_{6}$, $W_{2}$    --> 6 words\n",
    "\n",
    "$r_{3}$: \n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "$r_{N}$:\n",
    "\n",
    "<b>IDF($W_{i}$, $D_{c}$) = log(N/$n_{i}$)</b>, where N is the number of documents and $n_{i}$ is the number of documents which contain the word $W_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Since $n_{i}$ <= N, N/$n_{i}$ >= 1. So, log(N/$n_{i}$) >= 0</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. IDF >= 0\n",
    "2. If $n_{i}$ increases, then N/$n_{i}$ decreases. Here monotonic function log(N/$n_{i}$) decreases. \n",
    "<b>If $W_{i}$ is more frequent in $D_{c}$, the IDF is lower.</b> Hence, if IDF increases, $n_{i}$ decreases and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>If $W_{i}$ is more frequent, IDF will be low and if $W_{i}$ is very rare, IDF will be high.</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given documents {$r_{1}$, $r_{2}$, $r_{3}$,..., $r_{j}$} in $D_{c}$, <b>TF-IDF: TF($W_{i}$, $r_{j}$) * IDF($W_{i}$, $D_{c}$)</b>, TF($W_{i}$, $r_{j}$) is higher if $W_{i}$ is frequent in $r_{j}$ and IDF($W_{i}$, $D_{c}$) is higher when $W_{i}$ is rare in $D_{c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TF-IDF gives\n",
    "- gives more importance to rarer words in $D_{c}$.\n",
    "- gives more importance if a word is more frequent in a document/review.</b>\n",
    "\n",
    "But TF-IDF has a <u>drawback</u> that it <i><b>does not</b> take semantic meaning of words</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf=tf_idf_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910192"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=tf_idf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ales until',\n",
       " 'ales ve',\n",
       " 'ales would',\n",
       " 'ales you',\n",
       " 'alessandra',\n",
       " 'alessandra ambrosia',\n",
       " 'alessi',\n",
       " 'alessi added',\n",
       " 'alessi also',\n",
       " 'alessi and']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1268863)\t0.08275523980718573\n",
      "  (0, 1322643)\t0.05736320423975409\n",
      "  (0, 1181493)\t0.05904930622559946\n",
      "  (0, 2815806)\t0.06353114992572337\n",
      "  (0, 1562605)\t0.12941275579745923\n",
      "  (0, 1032033)\t0.11573055882069516\n",
      "  (0, 2075535)\t0.12941275579745923\n",
      "  (0, 2616462)\t0.12541096971801555\n",
      "  (0, 49126)\t0.04475667962178877\n",
      "  (0, 283308)\t0.054444217381069276\n",
      "  (0, 2381603)\t0.0769491583240197\n",
      "  (0, 2837880)\t0.07781013221409352\n",
      "  (0, 2324657)\t0.0937743722525583\n",
      "  (0, 324043)\t0.11573055882069516\n",
      "  (0, 2608442)\t0.10301992784348538\n",
      "  (0, 2838349)\t0.06715422842681543\n",
      "  (0, 126268)\t0.0893377308667213\n",
      "  (0, 361978)\t0.11856987122963351\n",
      "  (0, 552188)\t0.12941275579745923\n",
      "  (0, 1319489)\t0.0927612148772383\n",
      "  (0, 2578785)\t0.026787698091940104\n",
      "  (0, 105321)\t0.06840813514421003\n",
      "  (0, 1333169)\t0.053575946913953454\n",
      "  (0, 1739789)\t0.046544793491389236\n",
      "  (0, 2255427)\t0.12941275579745923\n",
      "  :\t:\n",
      "  (364170, 1254867)\t0.03283894855017248\n",
      "  (364170, 2614666)\t0.05504239102854844\n",
      "  (364170, 2897041)\t0.040848095932348116\n",
      "  (364170, 2550221)\t0.045449354719254265\n",
      "  (364170, 2771454)\t0.02772432091825306\n",
      "  (364170, 2745180)\t0.031851743745175284\n",
      "  (364170, 1169043)\t0.025105274169707193\n",
      "  (364170, 1384531)\t0.05960097701505191\n",
      "  (364170, 2312048)\t0.0378824455359619\n",
      "  (364170, 1063741)\t0.03709433849076868\n",
      "  (364170, 2891706)\t0.02700318583120386\n",
      "  (364170, 2592197)\t0.05406095709752581\n",
      "  (364170, 2843637)\t0.024866018323416738\n",
      "  (364170, 1091864)\t0.029888256751750948\n",
      "  (364170, 2545801)\t0.06524339220457043\n",
      "  (364170, 1647825)\t0.06902525115271677\n",
      "  (364170, 577030)\t0.08919740961914537\n",
      "  (364170, 2606992)\t0.10637648698033227\n",
      "  (364170, 1180506)\t0.1515904332736846\n",
      "  (364170, 139736)\t0.03135077437796279\n",
      "  (364170, 1332766)\t0.03625919264341876\n",
      "  (364170, 218560)\t0.031370049645728135\n",
      "  (364170, 2323768)\t0.0650989545513513\n",
      "  (364170, 1639344)\t0.022562482801020776\n",
      "  (364170, 2574871)\t0.037066476485010325\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert a row in sparse matrix to numpy array\n",
    "print(final_tf_idf)\n",
    "print(final_tf_idf[3,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n = 25):\n",
    "    '''Get top n tfidf values in row and return them with their corresponding values'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_features = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_features)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_features(final_tf_idf[1,:].toarray()[0], features, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sendak books</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosie movie</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paperbacks seem</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these sendak</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the paperbacks</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pages open</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>really rosie</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>incorporates them</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paperbacks</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>however miss</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seem kind</td>\n",
       "      <td>0.161317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up reading</td>\n",
       "      <td>0.156867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that incorporates</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the pages</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sendak</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rosie</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>of flimsy</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two hands</td>\n",
       "      <td>0.145130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>movie that</td>\n",
       "      <td>0.144374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reading these</td>\n",
       "      <td>0.137184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>too do</td>\n",
       "      <td>0.134491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>incorporates</td>\n",
       "      <td>0.134147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flimsy and</td>\n",
       "      <td>0.132254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature     tfidf\n",
       "0        sendak books  0.173437\n",
       "1         rosie movie  0.173437\n",
       "2     paperbacks seem  0.173437\n",
       "3       cover version  0.173437\n",
       "4        these sendak  0.173437\n",
       "5      the paperbacks  0.173437\n",
       "6          pages open  0.173437\n",
       "7        really rosie  0.168074\n",
       "8   incorporates them  0.168074\n",
       "9          paperbacks  0.168074\n",
       "10       however miss  0.164269\n",
       "11         hard cover  0.164269\n",
       "12          seem kind  0.161317\n",
       "13         up reading  0.156867\n",
       "14  that incorporates  0.155100\n",
       "15          the pages  0.149737\n",
       "16             sendak  0.149737\n",
       "17              rosie  0.146786\n",
       "18          of flimsy  0.146786\n",
       "19          two hands  0.145130\n",
       "20         movie that  0.144374\n",
       "21      reading these  0.137184\n",
       "22             too do  0.134491\n",
       "23       incorporates  0.134147\n",
       "24         flimsy and  0.132254"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word2Vec</b> takes semantic meaning of words into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm takes a word and converts it into a d-D vector where d is typically, 50, 100, 200 or 300. But this is not a sparse vector. But BoW / TF-IDF represented sentences into sparse vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 300-D vector. The higher the dimensions, more powerful is the representation.\n",
    "\n",
    "1. If $W_{1}$ and $W_{2}$ are semantically similar, then $v_{1}$ and $v_{2}$ are closer.\n",
    "2. In Word2Vec, it satisfies the relationships. \n",
    "\n",
    "<b>($V_{man}$ - $V_{woman}$) || ($V_{king}$ - $V_{queen}$) </b>\n",
    "\n",
    "Word2Vec learns relationships automatically from raw-text.\n",
    "\n",
    "Word2Vec takes a very large text Corpus as input and for every word it builds a vector. \n",
    "\n",
    "Larger dimensions --> more information rich the vector is. If we have a higher dimensional vector it can learn far more complex relationships. \n",
    "\n",
    "If $D_{c}$ is large, the higher is the dimensionality. \n",
    "\n",
    "Word2Vec looks at sequence information of words. Intuitively, for any word Word2Vec looks at neighborhood of that word. \n",
    "\n",
    "N($W_{i}$) is very similar to N($W_{j}$), then $v_{i}$ is very similar to $v_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "i = 0\n",
    "list_of_sentences = []\n",
    "for sentence in final['Text'].values:\n",
    "    filtered_sentence = []\n",
    "    sentence = cleanhtml(sentence)\n",
    "    for w in sentence.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):\n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue\n",
    "    list_of_sentences.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this witty little book makes my son laugh at loud. i recite it in the car as we're driving along and he always can sing the refrain. he's learned about whales, India, drooping roses:  i love all the new words this book  introduces and the silliness of it all.  this is a classic book i am  willing to bet my son will STILL be able to recite from memory when he is  in college\n",
      "----------------------------\n",
      "['this', 'witty', 'little', 'book', 'makes', 'my', 'son', 'laugh', 'at', 'loud', 'i', 'recite', 'it', 'in', 'the', 'car', 'as', 'were', 'driving', 'along', 'and', 'he', 'always', 'can', 'sing', 'the', 'refrain', 'hes', 'learned', 'about', 'whales', 'india', 'drooping', 'i', 'love', 'all', 'the', 'new', 'words', 'this', 'book', 'introduces', 'and', 'the', 'silliness', 'of', 'it', 'all', 'this', 'is', 'a', 'classic', 'book', 'i', 'am', 'willing', 'to', 'bet', 'my', 'son', 'will', 'still', 'be', 'able', 'to', 'recite', 'from', 'memory', 'when', 'he', 'is', 'in', 'college']\n"
     ]
    }
   ],
   "source": [
    "print(final['Text'].values[0])\n",
    "print('----------------------------')\n",
    "print(list_of_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I grew up reading these Sendak books, and watching the Really Rosie movie that incorporates them, and love them. My son loves them too. I do however, miss the hard cover version. The paperbacks seem kind of flimsy and it takes two hands to keep the pages open.\n",
      "----------------------------\n",
      "['i', 'grew', 'up', 'reading', 'these', 'sendak', 'books', 'and', 'watching', 'the', 'really', 'rosie', 'movie', 'that', 'incorporates', 'them', 'and', 'love', 'them', 'my', 'son', 'loves', 'them', 'too', 'i', 'do', 'however', 'miss', 'the', 'hard', 'cover', 'version', 'the', 'paperbacks', 'seem', 'kind', 'of', 'flimsy', 'and', 'it', 'takes', 'two', 'hands', 'to', 'keep', 'the', 'pages', 'open']\n"
     ]
    }
   ],
   "source": [
    "print(final['Text'].values[1])\n",
    "print('----------------------------')\n",
    "print(list_of_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(list_of_sentences, min_count = 5, vector_size = 50, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33521\n"
     ]
    }
   ],
   "source": [
    "words = list(w2v_model.wv.index_to_key)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tastey', 0.9309494495391846),\n",
       " ('yummy', 0.8578251004219055),\n",
       " ('satisfying', 0.8419342637062073),\n",
       " ('filling', 0.8285698890686035),\n",
       " ('delicious', 0.8193663954734802),\n",
       " ('flavorful', 0.7889677286148071),\n",
       " ('nutritious', 0.7660471200942993),\n",
       " ('delish', 0.7587419748306274),\n",
       " ('addicting', 0.756447970867157),\n",
       " ('versatile', 0.7515951991081238)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resemble', 0.6977817416191101),\n",
       " ('mean', 0.6769933700561523),\n",
       " ('dislike', 0.6450912952423096),\n",
       " ('prefer', 0.6284522414207458),\n",
       " ('overpower', 0.6175543069839478),\n",
       " ('think', 0.6041595339775085),\n",
       " ('enjoy', 0.5950704216957092),\n",
       " ('expect', 0.5933476090431213),\n",
       " ('miss', 0.576910674571991),\n",
       " ('fake', 0.5741640329360962)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity great\n"
     ]
    }
   ],
   "source": [
    "count_vect_feature = count_vect.get_feature_names()\n",
    "count_vect_feature.index('like')\n",
    "print(count_vect_feature[64055])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg-Word2Vec, tf-idf weighted Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Avg-Word2Vec </h4>\n",
    "\n",
    "Word2Vec takes a word and converts it into a d-D vector. \n",
    "\n",
    "But $r_{i}$ is a sequence of words/sentences.\n",
    "\n",
    "How do I convert my sentences to a vector using Word2Vec?\n",
    "\n",
    "Suppose we have a review $r_{1}$ containing words\n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$\n",
    "\n",
    "Suppose I want to convert $r_{1}$ to $v_{1}$, take the Avg Word2Vec representation. \n",
    "Take the first word $W_{1}$, convert it into a vector as $v_{1}$\n",
    "\n",
    "For each word in $r_{1}$, I am getting a vector representation.\n",
    "\n",
    "W2V($W_{1}$) + W2V($W_{2}$) + W2V($W_{1}$) + W2V($W_{3}$) + W2V($W_{4}$) + W2V($W_{5}$)\n",
    "\n",
    "Each of these vectors will be d-D. Add all these vectors and then divide the sum by the number of words. \n",
    "\n",
    "Suppose in $r_{1}$, there are $n_{1}$ words then $v_{1}$ becomes <b>1/$n_{1}$[W2V($W_{1}$) + W2V($W_{2}$) + W2V($W_{1}$) + W2V($W_{3}$) + W2V($W_{4}$) + W2V($W_{5}$)]</b>\n",
    "\n",
    "$v_{1}$ is the vector representation of review $r_{1}$. This is known as Avg-Word2Vec. It is not perfect but it works well. This is the simplest way to leverage Word2Vec to build sentence vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> tf-idf weighted Word2Vec </h4>\n",
    "\n",
    "Suppose we have a review $r_{1}$ containing words\n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$\n",
    "\n",
    "We compute tf-idf for $r_{1}$ as $t_{1}$, $t_{2}$, $t_{3}$, $t_{4}$, $t_{5}$.\n",
    "\n",
    "When we compute tf-idf-weighted Word2Vec of $r_{1}$\n",
    "\n",
    "<b>tfidf-W2V($r_{1}$) = [$t_{1}$ * W2V($W_{1}$) + $t_{2}$ * W2V($W_{2}$) + $t_{3}$ * W2V($W_{3}$) + $t_{4}$ * W2V($W_{4}$) + $t_{5}$ * W2V($W_{5}$)] / ($t_{1}$ + $t_{2}$ + $t_{3}$ + $t_{4}$ + $t_{5}$)</b> where $t_{i}$ is tf-idf of the word $w_{i}$ in review $r_{1}$ or <b> $t_{i}$ = tf-idf($w_{i}$, $r_{1}$)\n",
    "    \n",
    "    \n",
    "<b>If all $t_{i}$'s are 1, then tfidf-W2V is same as Avg-Word2Vec.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Avg-Word2Vec and tf-idf weighted Word2Vec are simple weighting strategies to convert sentences/paragraphs to vectors.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/8sq6h5px3mg90rq964p20c_00000gn/T/ipykernel_1400/59279885.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sent_vec/=cnt_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Average word2vec\n",
    "# Compute average word2vec for each review\n",
    "sent_vectors=[]\n",
    "for sent in list_of_sentences:\n",
    "    sent_vec=np.zeros(50)\n",
    "    cnt_words=0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.wv[word]\n",
    "            sent_vec+=vec\n",
    "            cnt_words+=1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec/=cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat=tf_idf_vect.get_feature_names()\n",
    "\n",
    "tfidf_sent_vectors=[]\n",
    "row=0\n",
    "for sent in list_of_sentences:\n",
    "    sent_vec=np.zeros(50)\n",
    "    weight_sum=0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.wv[word]\n",
    "\n",
    "            tfidf =final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec+=(vec*tfidf)\n",
    "            weight_sum+=tfidf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec/=weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row+=1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
